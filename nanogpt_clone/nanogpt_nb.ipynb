{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d34c0cd1b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# for reproducibility\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open dataset\n",
    "with open('tiny_shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    shakespeare = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# read part of dataset\n",
    "print(shakespeare[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# get characters used in dataset (and vocabulary size)\n",
    "chars = sorted(list(set(shakespeare)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and decode characters as integers\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42, 2]\n",
      "Hello world!\n"
     ]
    }
   ],
   "source": [
    "# testing char-int encoding and decoding\n",
    "print(encode(\"Hello world!\"))\n",
    "print(decode(encode(\"Hello world!\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode dataset in int-tensor\n",
    "data = torch.tensor(encode(shakespeare), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003854\n",
      "111540\n"
     ]
    }
   ],
   "source": [
    "# train test split\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(len(train_data))\n",
    "print(len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target is: 47\n",
      "when input is tensor([18, 47]) the target is: 56\n",
      "when input is tensor([18, 47, 56]) the target is: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target is: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target is: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target is: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is: 58\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58]) the target is: 47\n"
     ]
    }
   ],
   "source": [
    "# looking at feature/target blocks\n",
    "block_size = 9\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'when input is {context} the target is: {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be training the transformer on all of the above examples. We want the model to be able to recognize and make predictions for as little as a single word, and as much as an entire block size.\n",
    "\n",
    "This is not at the cost of computation, since these chunks will be processed in parallel in the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4 # sequences to process in parallel\n",
    "block_size = 8 # maximum sequence length to process\n",
    "\n",
    "def get_batch(data):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = get_batch(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "-----\n",
      "when input is [24] the target is: 43\n",
      "when input is [24, 43] the target is: 58\n",
      "when input is [24, 43, 58] the target is: 5\n",
      "when input is [24, 43, 58, 5] the target is: 57\n",
      "when input is [24, 43, 58, 5, 57] the target is: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target is: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target is: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target is: 39\n",
      "when input is [44] the target is: 53\n",
      "when input is [44, 53] the target is: 56\n",
      "when input is [44, 53, 56] the target is: 1\n",
      "when input is [44, 53, 56, 1] the target is: 58\n",
      "when input is [44, 53, 56, 1, 58] the target is: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target is: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target is: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target is: 1\n"
     ]
    }
   ],
   "source": [
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "print('-----')\n",
    "for b in range(2):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bigram language model uses an embedding table of size `(vocab_size, vocab_size)`, where passing the index pulls the corresponding row from the table.\n",
    "\n",
    "PyTorch arranges the bigram table into a `(B, T, C)` table, where: <br>\n",
    "B = Batch (batch_size) <br>\n",
    "T = Time (block_size) <br>\n",
    "C = Channel (vocab_size) <br>\n",
    "\n",
    "The Bigram language model does NOT consider sequences, only a single previous character (block_size == 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    # initialize (embed)\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are (B, T) of table\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "        # generation\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else: # training\n",
    "            # cross_entropy takes (B*T,C) so logits and targets need to be reshaped\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    # generate next set of tokens up to max_new_tokens\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(5.0364, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "l-QYjt'CL?jLDuQcLzy'RIo;'KdhpV\n",
      "vLixa,nswYZwLEPS'ptIZqOZJ$CA$zy-QTkeMk x.gQSFCLg!iW3fO!3DGXAqTsq3pdgq\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss= m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# start sequence with newline character\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "# sequence and generate (untrained model, random / garbage text)\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 15000 | loss: 4.6583\n",
      "1000 / 15000 | loss: 3.7861\n",
      "2000 / 15000 | loss: 3.1343\n",
      "3000 / 15000 | loss: 2.6584\n",
      "4000 / 15000 | loss: 2.6051\n",
      "5000 / 15000 | loss: 2.5792\n",
      "6000 / 15000 | loss: 2.5388\n",
      "7000 / 15000 | loss: 2.4331\n",
      "8000 / 15000 | loss: 2.4081\n",
      "9000 / 15000 | loss: 2.4967\n",
      "10000 / 15000 | loss: 2.4376\n",
      "11000 / 15000 | loss: 2.4759\n",
      "12000 / 15000 | loss: 2.4248\n",
      "13000 / 15000 | loss: 2.4868\n",
      "14000 / 15000 | loss: 2.3344\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "batch_size = 32\n",
    "total_steps = 15_000\n",
    "for steps in range(total_steps):\n",
    "    # sample batch of data\n",
    "    xb, yb = get_batch(train_data)\n",
    "    # evaluate loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    # zero gradients and backpropagate\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % 1_000==0:\n",
    "        print(f'{steps} / {total_steps} | loss: {round(loss.item(), 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints n_tokens of generated text\n",
    "def generate_text(n_tokens):\n",
    "    print(decode(\n",
    "        m.generate(\n",
    "            torch.zeros((1, 1), dtype=torch.long),\n",
    "            max_new_tokens=n_tokens\n",
    "        )[0].tolist()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEY ishwarod, se ttha's; I ppry memitth we ieeelo me,\n",
      "Fait Cloog se tt afre ce tim wary stuklel d lofran VID g\n",
      "OLI'tof areris nde imowlmandise wineatingiomanh y Mave,\n",
      "NCHegr.\n",
      "Trs y thaurymeresththonglast ffomofo, thiles's Ble t ireowor-mito shigee mer\n",
      "Wank\n",
      "Myo I t Wh wiakn I ad y, LICophouplefarouk \n"
     ]
    }
   ],
   "source": [
    "generate_text(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a math trick in self-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n",
      "values\n",
      "tensor([[ 1.8236, -1.7576],\n",
      "        [-0.5178, -1.2025],\n",
      "        [ 0.6484,  1.6954],\n",
      "        [-0.8710, -1.2504],\n",
      "        [ 0.9018, -0.0196],\n",
      "        [-0.4445,  0.5347],\n",
      "        [ 1.3271,  0.0285],\n",
      "        [-0.3768, -0.5363]])\n",
      "averages\n",
      "tensor([[ 1.8236, -1.7576],\n",
      "        [ 0.6529, -1.4801],\n",
      "        [ 0.6514, -0.4216],\n",
      "        [ 0.2708, -0.6288],\n",
      "        [ 0.3970, -0.5069],\n",
      "        [ 0.2568, -0.3333],\n",
      "        [ 0.4097, -0.2816],\n",
      "        [ 0.3113, -0.3135]])\n"
     ]
    }
   ],
   "source": [
    "# matrix dimensions\n",
    "B,T,C = 4,8,2\n",
    "# random (weights or values)\n",
    "x = torch.randn(B,T,C)\n",
    "print(x.shape)\n",
    "# \"bag of words\" (average of words)\n",
    "# average of all previous words & current word\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1]\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n",
    "print('values')\n",
    "print(x[0])\n",
    "print('averages')\n",
    "print(xbow[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The for loop above is inefficient. The 'mathematical trick' is a form of matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "-----\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "-----\n",
      "c=\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# triangular 3x3 ones\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "# random 3x2 of integers\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "# multiply\n",
    "c = a @ b\n",
    "\n",
    "print('a=')\n",
    "print(a)\n",
    "print('-----')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('-----')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is for summing values, but we can do the same for averages by averaging the triangular matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "-----\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "-----\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# triangular 3x3 ones\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "# random 3x2 of integers\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "# multiply\n",
    "c = a @ b\n",
    "\n",
    "print('a=')\n",
    "print(a)\n",
    "print('-----')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('-----')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to the toy example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# triangular average matrix\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "print(wei)\n",
    "# multiplying by toy matrix\n",
    "# (T, T) @ (B, T, C) --> broadcast\n",
    "# (B, T, T) @ (B, T, C)\n",
    "# = (B, T, C)\n",
    "xbow2 = wei @ x\n",
    "# confirm equality of nested for loop and matrix mult\n",
    "print(torch.allclose(xbow, xbow2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another way to do this: use softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "tensor([[ 1.8236, -1.7576],\n",
      "        [ 0.6529, -1.4801],\n",
      "        [ 0.6514, -0.4216],\n",
      "        [ 0.2708, -0.6288],\n",
      "        [ 0.3970, -0.5069],\n",
      "        [ 0.2568, -0.3333],\n",
      "        [ 0.4097, -0.2816],\n",
      "        [ 0.3113, -0.3135]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# lower triangular ones\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "print(tril)\n",
    "# all zeros\n",
    "wei = torch.zeros((T, T))\n",
    "# lower triangular zeros, upper triangular -inf\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "print(wei)\n",
    "# triangular average matrix\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "print(wei)\n",
    "# matmul\n",
    "xbow3 = wei @ x\n",
    "print(xbow3[0])\n",
    "# confirm equality\n",
    "print(torch.allclose(xbow, xbow3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement self-attention. This is no longer a simple average of the previous tokens; rather, we can have data-driven connections between the final token and any of the previous tokens in the sequence.\n",
    "\n",
    "We can implement this with the bread and butter of self-attention: the query and key vectors. Each token has a query vector (what am I looking for?) and a key vector (what do I contain?). The query vectors dot product with the key vectors for each token, so tokens that have a stronger interaction have a larger dot product. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{Attention}(Q,K,V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V $$\n",
    "\n",
    "where $d_k$ is `head_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "head_size = 16 # H\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B,T,H)\n",
    "q = query(x) # (B,T,H)\n",
    "# query / key dot product\n",
    "wei = q @ k.transpose(-2, -1) # (B,T,H) @ (B,H,T) --> (B,T,T)\n",
    "wei *= head_size**-0.5\n",
    "\n",
    "# softmax\n",
    "tril = torch.tril(torch.ones((T,T)))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "# communication between query / key and value\n",
    "out = wei @ v # (B,T,H)\n",
    "print(out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
